{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Chatbot_V3_20202310.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SBauS7a1ZxRb",
        "outputId": "e7f1fa44-9fe2-4ebe-bf41-cdba318b7c9a"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RC1mcP8mZti8"
      },
      "source": [
        "### Install required packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0q2Mz_sZIwPH"
      },
      "source": [
        "!pip install simpletransformers\n",
        "!pip install transformers[\"ja\"]\n",
        "!pip install -U spacy\n",
        "!python -m spacy download ja_core_news_lg\n",
        "!pip install -U sentence-transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16DMmyKsZyrY"
      },
      "source": [
        "### Load requried packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3S_FZwk6Z3eK"
      },
      "source": [
        "from simpletransformers.classification import ClassificationModel, ClassificationArgs\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from simpletransformers.seq2seq import Seq2SeqModel\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import spacy\n",
        "import requests\n",
        "import datetime as dt\n",
        "import random\n",
        "import time\n",
        "import torch\n",
        "from scipy.special import softmax\n",
        "import logging\n",
        "from sklearn.utils import shuffle\n",
        "import pickle\n",
        "import json\n",
        "import os\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5zzTR-nRQH1B"
      },
      "source": [
        "## Accessories"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yUVzexE8QKhl"
      },
      "source": [
        "FILES_DIRECTORY = './drive/MyDrive/chatbot/'"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pgh3zgiHopRY"
      },
      "source": [
        "### Data pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nmaSyuVcoo8T"
      },
      "source": [
        "# read excel file\n",
        "data = pd.read_excel(FILES_DIRECTORY + '/train_intent.xlsx')\n",
        "data.head()\n",
        "data = shuffle(data)\n",
        "# Get all intents as list\n",
        "intents = data['intent'].unique().tolist()\n",
        "# map labels with index of the list\n",
        "data['map labels']  = data.intent.map(lambda v: intents.index(v))\n",
        "#Intents training data [\"text\", \"labels\"]\n",
        "train_data_intents = data[['Q', 'map labels']].rename(columns={'Q': \"text\",\n",
        "                                                      'map labels':\"labels\"})\n",
        "train_data_general = data[data['intent']=='general-enquiry'][['Q', 'A']].rename(columns={'Q': \"input_text\", \n",
        "                                                                                 'A':\"target_text\"})\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HnOs8Pr7Q_Iz"
      },
      "source": [
        "# Save intents\n",
        "with open(FILES_DIRECTORY +'/intents.json', 'w') as file:\n",
        "    json.dump(intents, file)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gE5exCtlao_a"
      },
      "source": [
        "### Intent Recognition Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_mfKwfxBavz0"
      },
      "source": [
        "#### Training the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v6sCIPZSJHcX"
      },
      "source": [
        "logging.basicConfig(level=logging.INFO)\n",
        "transformers_logger = logging.getLogger(\"transformers\")\n",
        "transformers_logger.setLevel(logging.WARNING)\n",
        "\n",
        "\n",
        "#defining paramenters\n",
        "model_type = \"bert\" \n",
        "model_name = \"cl-tohoku/bert-base-japanese-whole-word-masking\"\n",
        "output_directory = FILES_DIRECTORY + '/models/model_intent/'\n",
        "cuda_available = torch.cuda.is_available()\n",
        "hide_progress = False\n",
        "\n",
        "model_args = {\n",
        "    \"reprocess_input_data\": True,\n",
        "    \"output_dir\": output_directory,\n",
        "    \"overwrite_output_dir\": True,\n",
        "    \"train_batch_size\": 32,\n",
        "    \"num_train_epochs\": 50,\n",
        "    \"save_eval_checkpoints\": False,\n",
        "    \"save_steps\": -1,\n",
        "    \"save_model_every_epoch\": False,\n",
        "    \"use_multiprocessing\": False,\n",
        "    \"manual_seed\": 4,\n",
        "    \"no_cache\": True,\n",
        "    \"evaluate_during_training_steps\": 2000,\n",
        "    \"silent\": hide_progress,\n",
        "}\n",
        "\n",
        "\n",
        "model = ClassificationModel(\n",
        "    model_type,\n",
        "    model_name,\n",
        "    num_labels=len(intents),\n",
        "    args=model_args,\n",
        "    use_cuda=cuda_available\n",
        ") \n",
        "\n",
        "#training the model\n",
        "model.train_model(train_data_intents)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Euzd1EivdIuR"
      },
      "source": [
        "#### Loading saved model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ywRhXHyydLFZ"
      },
      "source": [
        "# defining accessories\n",
        "cuda_available = torch.cuda.is_available()\n",
        "model_directory = FILES_DIRECTORY + \"/models/model_intent\"\n",
        "model_args = FILES_DIRECTORY + \"/models/model_intent/model_args.json\" \n",
        "model_type =  \"bert\"\n",
        "\n",
        "model_intent = ClassificationModel(\n",
        "    model_type,\n",
        "    model_name = model_directory,\n",
        "    args=model_args,\n",
        "    use_cuda=cuda_available,\n",
        ")\n",
        "\n",
        "with open(FILES_DIRECTORY + '/intents.json') as file:\n",
        "   intents = json.load(file)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2c-50QYh9R2R"
      },
      "source": [
        "### Predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GvjRasYq8Mk6"
      },
      "source": [
        "def predict_intent(sentence, model_intent):\n",
        "  model_intent.args.silent= True\n",
        "  model_intent.args.use_multiprocessing_for_evaluation = False \n",
        "  predictions, raw_outputs = model_intent.predict([sentence])\n",
        "  probabilities = softmax(raw_outputs, axis=1)\n",
        "  results = {'label': intents[predictions[0]], \n",
        "           'score': probabilities[0][predictions[0]] }\n",
        "  return results"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_TShtx7u9yt-"
      },
      "source": [
        "### Entity Extraction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vXLCAChV3p52"
      },
      "source": [
        "def get_entities(text):\n",
        "  nlp = spacy.load('ja_core_news_lg') # add this while initialize the main\n",
        "  doc = nlp(text, disable=[\"tok2vec\", \"tagger\", \"parser\", \"attribute_ruler\", \"lemmatizer\"])\n",
        "  entities = {}\n",
        "  for ent in doc.ents:\n",
        "    entities[ent.text] = ent.label_\n",
        "\n",
        "  return entities"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nSZci-uZMZ8f"
      },
      "source": [
        "### Sentence BERT Transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5VjDPAYHOtrg"
      },
      "source": [
        "#### Queries embedding/training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dz7Zu-5TOSqh"
      },
      "source": [
        "model = SentenceTransformer('sentence-transformers/paraphrase-xlm-r-multilingual-v1')\n",
        "\n",
        "# get list of training queries and responses\n",
        "train_queries = train_data_general['input_text'].values.tolist()\n",
        "train_responses = train_data_general['target_text'].values.tolist()\n",
        "\n",
        "# encode queries\n",
        "embeddings = model.encode(train_queries, convert_to_tensor=True)\n",
        "\n",
        "emb_dir = FILES_DIRECTORY + \"/models/model_general/\"\n",
        "\n",
        "# Create if directory doesn't exists\n",
        "if not os.path.exists(emb_dir):\n",
        "    os.mkdir(emb_dir)\n",
        "\n",
        "#Store data & embeddings on disc\n",
        "with open(emb_dir + 'embeddings.pkl', \"wb\") as fOut:\n",
        "    pickle.dump({'queries': train_queries, 'embeddings': embeddings, 'responses': train_responses}, fOut, protocol=pickle.HIGHEST_PROTOCOL)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AgcnqGBCNvti"
      },
      "source": [
        "#### Loading Model with accessories"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DwqbaoZ2NyOt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5aa85c4-fa96-49cb-95ee-c76e48f023d3"
      },
      "source": [
        "model_general = SentenceTransformer('sentence-transformers/paraphrase-xlm-r-multilingual-v1')\n",
        "\n",
        "#Load data & embeddings from disc\n",
        "emb_dir = FILES_DIRECTORY + \"/models/model_general/embeddings.pkl\"\n",
        "with open(emb_dir, \"rb\") as fIn:\n",
        "    stored_data = pickle.load(fIn)\n",
        "    stored_queries = stored_data['queries']\n",
        "    stored_embeddings = stored_data['embeddings']\n",
        "    stored_responses = stored_data['responses']"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: sentence-transformers/paraphrase-xlm-r-multilingual-v1\n",
            "INFO:sentence_transformers.SentenceTransformer:Use pytorch device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54DXtyvStc2n"
      },
      "source": [
        "### Responses"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KMibwli7tcdF"
      },
      "source": [
        "def get_greeting_response():\n",
        "  greeting_responses = ['おはよう～！今日も元気？', \n",
        "                        'こんにちは、元気ですか？',\n",
        "                        'おはようございます、どうすればお手伝いできますか？',\n",
        "\n",
        "                        ] \n",
        "  return random.choice(greeting_responses) # select random response, TODO: Add more entries \n",
        "\n",
        "def get_weather_response(user_query):\n",
        "  date = '今日は、'\n",
        "  place = ''\n",
        "  days_list = ['月曜日', '火曜日', '水曜日', '木曜日', '金曜日', '土曜日', '日曜日']\n",
        "  entities = get_entities(user_query)\n",
        "  if 'GPE' in entities.values(): #TO DO: Improvement for weather forecasting etc..\n",
        "    # get first location  \n",
        "    location = [key for key in entities if (entities[key] == 'GPE')][0]\n",
        "    place = f'の{location}は、'\n",
        "  else:\n",
        "    location = '東京'\n",
        "  if 'DATE' in entities.values():\n",
        "    #[Monday, Sunday, ..]\n",
        "    days_list = ['月曜日', '火曜日', '水曜日', '木曜日', '金曜日', '土曜日', '日曜日']\n",
        "    date_ent = [key for key in entities if (entities[key] == 'DATE')][0]\n",
        "  else:\n",
        "    date_ent = {}\n",
        "    # today\n",
        "  if '今日' in user_query:\n",
        "    cnt = 0\n",
        "    date = '今日は、'\n",
        "  #tomorrow\n",
        "  elif '明日' in user_query:\n",
        "    cnt = 1\n",
        "    date = '明日は、'\n",
        "  # day after tomorrow\n",
        "  elif '明後日' in user_query:\n",
        "    cnt = 2\n",
        "    date = '明後日は'\n",
        "  elif '次の週' in user_query:\n",
        "    cnt = 7\n",
        "    date = '来週は、'\n",
        "  elif date_ent in days_list:\n",
        "    current_weekday = dt.datetime.today().weekday()\n",
        "    updated_days_list = days_list[current_weekday:] + days_list[:current_weekday]\n",
        "    cnt = updated_days_list.index(date_ent)\n",
        "    date = f'{date_ent}は、'\n",
        "  url = f\"http://api.openweathermap.org/data/2.5/weather?q={location}&lang=ja&appid=1c412139fa61a76f27f9683cf5937117\"\n",
        "  response = requests.get(url)\n",
        "  if response.status_code == 200:\n",
        "    minimum_temperature = response.json()['main']['temp_min']\n",
        "    max_temperature = response.json()['main']['temp_max']\n",
        "    weather = response.json()['weather'][0]['description']\n",
        "    return f'{date}{place}最高気温{max_temperature}度、最低気温{minimum_temperature}度、{weather}です。'\n",
        "  else:\n",
        "    # City not found!\n",
        "    return \"都市が見つかりません！\"\n",
        "  \n",
        "def get_time_response():\n",
        "  current_time = dt.datetime.now()\n",
        "  hour = current_time.strftime(\"%H\")\n",
        "  minutes = current_time.strftime(\"%M\")\n",
        "  return f'今の時間は{hour}時{minutes}分だよ' \n",
        "\n",
        "def get_alarm_response(user_query):\n",
        "  alarm_time = '30 秒'\n",
        "  print(f'{alarm_time}だね！今から数えるよ、スタート！')\n",
        "  # define the countdown func.\n",
        "  def countdown(t):\n",
        "    while t>-1:\n",
        "        mins, secs = divmod(t, 60)\n",
        "        timer = '{:02d}:{:02d}'.format(mins, secs)\n",
        "        print('\\r', timer, end=\"\")\n",
        "        time.sleep(1)\n",
        "        t -= 1\n",
        "  set_timer = 30     \n",
        "  # function call\n",
        "  countdown(int(set_timer))\n",
        "  return #f'{alarm_time}だね！今から数えるよ、スタート！'\n",
        "\n",
        "def get_date_response(user_query):\n",
        "  today = dt.datetime.today()\n",
        "  month = today.month\n",
        "  day = today.day\n",
        "  return f'今日は{month}月{day}日だよ！'\n",
        "\n",
        "def get_response_common_enquiry(user_query):\n",
        "  return 'おかえり！今日も楽しかったかな？'\n",
        "\n",
        "def get_response_friend_enquiry(user_query):\n",
        "  return 'おかえり！今日は{user_friend_name}と遊んだかな？'\n",
        "\n",
        "def get_response_teacher_enquiry(user_query):\n",
        "  return 'おかえり！今日は{user_teacher_name}先生と仲良くできた？'\n",
        "\n",
        "def get_response_school_enquiry(user_query):\n",
        "  return 'おかえり！今日は{user_aftershcool}の日だったかな？楽しかったかな？{robot_name}に聞かせてね！'\n",
        "\n",
        "def get_response_robot_gender_enquiry(user_query):\n",
        "  return 'みんな気になるよね～。あんまり教えたくはないんだけど、{robot_name}は{gender}なんだよね。'\n",
        "\n",
        "def get_response_robot_birthplace_enquiry(user_query):\n",
        "  return '{robot_name}は{birth_place}から来たんだよ！'\n",
        "\n",
        "def get_response_robot_age_enquiry(user_query):\n",
        "  return '{robot_name}は{user_age}だよ！{user_name}と同じかな？'\n",
        "\n",
        "def get_response_robot_favorite_enquiry(user_query):\n",
        "  return '{robot_name}は{robot_favorite}が好きなんだー！'\n",
        "\n",
        "def get_response_song_enquiry(user_query):\n",
        "  return '{robot_name}は歌を歌うの好きなんだ！{song_name}でも歌おうかなー！'\n",
        "\n",
        "def get_general_response(user_query, model, stored_embeddings, stored_responses):\n",
        "  query_embedding = model.encode([user_query], convert_to_tensor=True,  show_progress_bar= False)\n",
        "  hits = util.semantic_search(query_embedding, stored_embeddings, top_k=1)\n",
        "  hits = hits[0]\n",
        "  if hits[0]['score'] > 0.3:\n",
        "    response = stored_responses[hits[0]['corpus_id']]\n",
        "  else:\n",
        "    response = 'すみません、わかりません！'\n",
        "  return response\n",
        "\n",
        "#TO DO: DB integration + add  new intent responses ...."
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CmXS4UOZnDi3"
      },
      "source": [
        "## Actions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fGcr6-dXnGWg"
      },
      "source": [
        "def action(user_query, intent, score, model, stored_embeddings, stored_responses):\n",
        "  if (score < 0.4):\n",
        "    return '申し訳ありませんが、わかりません。'\n",
        "  elif (intent == 'time-enquiry'):\n",
        "    return get_time_response()\n",
        "  elif (intent == 'weather-enquiry'):\n",
        "    return get_weather_response(user_query)\n",
        "  elif (intent == 'alarm-enquiry'):\n",
        "    return get_alarm_response(user_query)\n",
        "  elif (intent == 'date-enquiry'):\n",
        "    return get_date_response(user_query)\n",
        "  elif (intent == 'response-common-enquiry'):\n",
        "    return get_response_common_enquiry(user_query)\n",
        "  elif (intent == 'response-friend-enquiry'):\n",
        "    return get_response_friend_enquiry(user_query)\n",
        "  elif (intent == 'response-teacher-enquiry'):\n",
        "    return get_response_teacher_enquiry(user_query)\n",
        "  elif (intent == 'response-school-enquiry'):\n",
        "    return get_response_school_enquiry(user_query)\n",
        "  elif (intent == 'robot-gender-enquiry'):\n",
        "    return get_response_robot_gender_enquiry(user_query)\n",
        "  elif (intent == 'robot-birthplace-enquiry'):\n",
        "    return get_response_robot_birthplace_enquiry(user_query)\n",
        "  elif (intent == 'robot-age-enquiry'):\n",
        "    return get_response_robot_age_enquiry(user_query)\n",
        "  elif (intent == 'robot-favorite-enquiry'):\n",
        "    return get_response_robot_favorite_enquiry(user_query)\n",
        "  elif (intent == 'song-enquiry'):\n",
        "    return get_response_song_enquiry(user_query)\n",
        "  elif (intent == 'general-enquiry'):\n",
        "    return get_general_response(user_query, model, stored_embeddings, stored_responses)  \n",
        "  else:\n",
        "    return 'This feature is coming soon!'\n",
        "#TO DO: Add more actions ..."
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1m4RXgM6iwIf"
      },
      "source": [
        "## Chatbot -- Interface"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DHt80YivY7aA",
        "outputId": "9eaebb34-4e29-4162-939b-575211ffb30c"
      },
      "source": [
        "# ChatBot\n",
        "bot_name = 'bot'\n",
        "user_name = 'user name'\n",
        "print(\"let's chat! Type quit to exit.\")\n",
        "while True:\n",
        "  sentence = input('you: ')\n",
        "  if sentence =='quit':\n",
        "    break\n",
        "  result = predict_intent(sentence, model_intent)\n",
        "  response = action(sentence, result['label'], result['score'], model_general, stored_embeddings, stored_responses)\n",
        "  # replace @user with user name\n",
        "  response = response.replace('@ユーザ', user_name)\n",
        "  print(f\"{bot_name}: {response}\")"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "let's chat! Type quit to exit.\n",
            "you: うん。あんまりできなかった。\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:simpletransformers.classification.classification_utils: Converting to features started. Cache is not used.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "bot: 難しかったんだね。わからなかったところは見直しておくといいよ。\n",
            "you: 明日の品川の天気は？\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:simpletransformers.classification.classification_utils: Converting to features started. Cache is not used.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "bot: 都市が見つかりません！\n",
            "you: 明後日の品川の天気教えて\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:simpletransformers.classification.classification_utils: Converting to features started. Cache is not used.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "bot: 都市が見つかりません！\n",
            "you: 天気はどう？\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:simpletransformers.classification.classification_utils: Converting to features started. Cache is not used.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "bot: 今日は、最高気温289.2度、最低気温285.59度、薄い雲です。\n",
            "you: 今日の天気はどう？\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:simpletransformers.classification.classification_utils: Converting to features started. Cache is not used.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "bot: 今日は、最高気温289.2度、最低気温285.59度、薄い雲です。\n",
            "you: どう？おいしいでしょ。\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:simpletransformers.classification.classification_utils: Converting to features started. Cache is not used.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "bot: うん。すごくおいしいよ。user nameーちゃんはお料理上手だね。\n",
            "you: 計算問題が苦手だな\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:simpletransformers.classification.classification_utils: Converting to features started. Cache is not used.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "bot: やさしい問題をまずはゆっくり解いてみよう！繰り返しやるとだんだんできるようになるよ。\n",
            "you: トイレ行かない\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:simpletransformers.classification.classification_utils: Converting to features started. Cache is not used.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "bot: 眠ってるときにトイレに行きたくなるかもしれないから、先に行ってから寝ようね。\n",
            "you: もったいないって何？\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:simpletransformers.classification.classification_utils: Converting to features started. Cache is not used.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "bot: 何か食べ残したり、使い残したりするとむだになってしまうから、そうならないようにしようねということだよ。\n",
            "you: quit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-zl88nmzLb4"
      },
      "source": [
        "### Prediction on whole General Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MTILcFfYI35M"
      },
      "source": [
        "# For the purpose of evaluation ... \n",
        "res_list = []\n",
        "for inp in train_data_general['input_text']:\n",
        "  res_list.append(get_general_response(inp, model, stored_embeddings, stored_responses))\n",
        "\n",
        "train_data_general['predicted'] = res_list\n",
        "# Save predicted response of general to excel\n",
        "train_data_general.to_excel('predicted.xlsx')"
      ],
      "execution_count": 21,
      "outputs": []
    }
  ]
}