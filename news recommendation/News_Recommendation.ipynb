{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# NEWS RECOMMENDATION\n",
        "\n",
        "Welcome to the notebook on news recommendation using NLP and recommendation systems! In today's world, there is an abundance of news articles available online, making it challenging for users to find the content that is most relevant to their interests. This is where news recommendation systems come in, providing personalized article suggestions to users based on their reading history and preferences.\n",
        "\n",
        "In this notebook, we will be using the Microsoft MIND dataset and the Microsoft Recommenders library to build a news recommendation system. Specifically, we will be implementing the LSTUR algorithm, which combines natural language processing techniques with recommendation systems to provide a more accurate and personalized experience for users.\n",
        "\n",
        "By the end of this notebook, you will have a deeper understanding of news recommendation systems and the tools and techniques used to build them. So, let's get started and dive into the world of news recommendation using NLP and recommendation systems!"
      ],
      "metadata": {
        "id": "vMPOTlSVeg-9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installing required packages"
      ],
      "metadata": {
        "id": "CqinS42CeoMF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1hc9TMfteX-O"
      },
      "outputs": [],
      "source": [
        "# installing microsoft recommenders library \n",
        "!pip install recommenders scrapbook"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing Required Packages"
      ],
      "metadata": {
        "id": "_oVrX8oHesRZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import zipfile\n",
        "from tqdm import tqdm\n",
        "import scrapbook as sb\n",
        "from tempfile import TemporaryDirectory\n",
        "import tensorflow as tf\n",
        "tf.get_logger().setLevel('ERROR') # only show error messages\n",
        "\n",
        "from recommenders.models.deeprec.deeprec_utils import download_deeprec_resources \n",
        "from recommenders.models.newsrec.newsrec_utils import prepare_hparams\n",
        "from recommenders.models.newsrec.models.lstur import LSTURModel\n",
        "from recommenders.models.newsrec.io.mind_iterator import MINDIterator\n",
        "from recommenders.models.newsrec.newsrec_utils import get_mind_data_set\n",
        "\n",
        "print(\"System version: {}\".format(sys.version))\n",
        "print(\"Tensorflow version: {}\".format(tf.__version__))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6zEprtNNeuCG",
        "outputId": "b8ee4f58-7535-42e1-945b-29ba54b5342c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "System version: 3.9.16 (main, Dec  7 2022, 01:11:51) \n",
            "[GCC 9.4.0]\n",
            "Tensorflow version: 2.11.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Collecting and Loading Data"
      ],
      "metadata": {
        "id": "KsH_GHRyfTo_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initializing parameters\n",
        "epochs = 5\n",
        "seed = 40\n",
        "batch_size = 32\n",
        "\n",
        "# Options: demo, small, large\n",
        "MIND_type = 'large'"
      ],
      "metadata": {
        "id": "OteJytFwexC2"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting data\n",
        "# First, we create a temporary directory to store the data for this notebook.\n",
        "tmpdir = TemporaryDirectory()\n",
        "data_path = tmpdir.name\n",
        "\n",
        "# We then define the file paths for the train news and behavior files, as well as the valid news and behavior files.\n",
        "train_news_file = os.path.join(data_path, 'train', r'news.tsv')\n",
        "train_behaviors_file = os.path.join(data_path, 'train', r'behaviors.tsv')\n",
        "valid_news_file = os.path.join(data_path, 'valid', r'news.tsv')\n",
        "valid_behaviors_file = os.path.join(data_path, 'valid', r'behaviors.tsv')\n",
        "\n",
        "# We also define the file paths for the word embeddings, user dictionary, word dictionary, and yaml file.\n",
        "wordEmb_file = os.path.join(data_path, \"utils\", \"embedding.npy\")\n",
        "userDict_file = os.path.join(data_path, \"utils\", \"uid2index.pkl\")\n",
        "wordDict_file = os.path.join(data_path, \"utils\", \"word_dict.pkl\")\n",
        "yaml_file = os.path.join(data_path, \"utils\", r'lstur.yaml')\n",
        "\n",
        "# We then call the get_mind_data_set function to get the MIND dataset, train and dev data, and utils file path.\n",
        "mind_url, mind_train_dataset, mind_dev_dataset, mind_utils = get_mind_data_set(MIND_type)\n",
        "\n",
        "# We then check if the train and valid news files exist. If they don't, we call the download_deeprec_resources\n",
        "# function to download the MIND dataset train and dev files to the corresponding directories.\n",
        "if not os.path.exists(train_news_file):\n",
        "    download_deeprec_resources(mind_url, os.path.join(data_path, 'train'), mind_train_dataset)\n",
        "    \n",
        "if not os.path.exists(valid_news_file):\n",
        "    download_deeprec_resources(mind_url, os.path.join(data_path, 'valid'), mind_dev_dataset)\n",
        "\n",
        "# We also check if the yaml file exists. If it doesn't, we call the download_deeprec_resources function to download\n",
        "# the lstur.yaml file to the utils directory.\n",
        "if not os.path.exists(yaml_file):\n",
        "    download_deeprec_resources(r'https://recodatasets.z20.web.core.windows.net/newsrec/', \\\n",
        "                               os.path.join(data_path, 'utils'), mind_utils)\n"
      ],
      "metadata": {
        "id": "vxJMeGUohlPT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exploratory Data Analysis"
      ],
      "metadata": {
        "id": "P85CO2H4gEo3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading news dataset, format: [News ID] [Category] [Subcategory] [News Title] [News Abstrct] [News Url] [Entities in News Title] [Entities in News Abstract]\n",
        "df_news = pd.read_table(train_news_file)\n",
        "# Display the top 4 rows \n",
        "print(df_news.head(4).to_string(header=False))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "swLgqopMf8rq",
        "outputId": "31cebca5-000c-491b-e84c-e90d93a54e7a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0  N10399     news          newsworld                      The Cost of Trump's Aid Freeze in the Trenches of Ukraine's War  Lt. Ivan Molchanets peeked over a parapet of sand bags at the front line of the war in Ukraine. Next to him was an empty helmet propped up to trick snipers, already perforated with multiple holes.                                       https://www.msn.com/en-us/news/world/the-cost-of-trumps-aid-freeze-in-the-trenches-of-ukraines-war/ar-AAJgNsz?ocid=chopendata                                                                                                                                                 []                                                                                                                                                                                                                                                                                                                    [{\"Label\": \"Ukraine\", \"Type\": \"G\", \"WikidataId\": \"Q212\", \"Confidence\": 0.946, \"OccurrenceOffsets\": [87], \"SurfaceForms\": [\"Ukraine\"]}]\n",
            "1  N12103   health             voices                          I Was An NBA Wife. Here's How It Affected My Mental Health.                                                                                                   I felt like I was a fraud, and being an NBA wife didn't help that. In fact, it nearly destroyed me.                                         https://www.msn.com/en-us/health/voices/i-was-an-nba-wife-heres-how-it-affected-my-mental-health/ar-AACk2N6?ocid=chopendata                                                                                                                                                 []                                                                                                                                                                                                                                                                                               [{\"Label\": \"National Basketball Association\", \"Type\": \"O\", \"WikidataId\": \"Q155223\", \"Confidence\": 1.0, \"OccurrenceOffsets\": [40], \"SurfaceForms\": [\"NBA\"]}]\n",
            "2  N20460   health            medical                            How to Get Rid of Skin Tags, According to a Dermatologist                   They seem harmless, but there's a very good reason you shouldn't ignore them. The post How to Get Rid of Skin Tags, According to a Dermatologist appeared first on Reader's Digest.                                       https://www.msn.com/en-us/health/medical/how-to-get-rid-of-skin-tags,-according-to-a-dermatologist/ar-AAAKEkt?ocid=chopendata        [{\"Label\": \"Skin tag\", \"Type\": \"C\", \"WikidataId\": \"Q3179593\", \"Confidence\": 1.0, \"OccurrenceOffsets\": [18], \"SurfaceForms\": [\"Skin Tags\"]}]  [{\"Label\": \"Skin tag\", \"Type\": \"C\", \"WikidataId\": \"Q3179593\", \"Confidence\": 1.0, \"OccurrenceOffsets\": [105], \"SurfaceForms\": [\"Skin Tags\"]}, {\"Label\": \"Dermatology\", \"Type\": \"C\", \"WikidataId\": \"Q171171\", \"Confidence\": 1.0, \"OccurrenceOffsets\": [131], \"SurfaceForms\": [\"Dermatologist\"]}, {\"Label\": \"Reader's Digest\", \"Type\": \"M\", \"WikidataId\": \"Q371820\", \"Confidence\": 0.999, \"OccurrenceOffsets\": [163], \"SurfaceForms\": [\"Reader's Digest\"]}]\n",
            "3   N5409  weather  weathertopstories  It's been Orlando's hottest October ever so far, but cooler temperatures on the way                                                                           There won't be a chill down to your bones this Halloween in Orlando, unless you count the sweat dripping from your armpits.  https://www.msn.com/en-us/weather/weathertopstories/it's-been-orlando's-hottest-october-ever-so-far,-but-cooler-temperatures-on-the-way/ar-AAJwoxD?ocid=chopendata  [{\"Label\": \"Orlando, Florida\", \"Type\": \"G\", \"WikidataId\": \"Q49233\", \"Confidence\": 0.962, \"OccurrenceOffsets\": [10], \"SurfaceForms\": [\"Orlando\"]}]                                                                                                                                                                                                                                                                                                         [{\"Label\": \"Orlando, Florida\", \"Type\": \"G\", \"WikidataId\": \"Q49233\", \"Confidence\": 0.962, \"OccurrenceOffsets\": [60], \"SurfaceForms\": [\"Orlando\"]}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# loading behaviors dataset, format: [Impression ID] [User ID] [Impression Time] [User Click History] [Impression News]\n",
        "df_behaviors = pd.read_table(train_behaviors_file)\n",
        "# Display the top 4 rows \n",
        "print(df_behaviors.head(4).to_string(header=False))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SPtvBQc9g8Dh",
        "outputId": "9165e56b-91bd-4838-e328-d197167fdfe8"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0  2  U84185  11/12/2019 10:36:47 AM                                                                                                                                                                                                                                                                                                                                                           N27209 N11723 N4617 N12320 N11333 N24461 N22111 N14026 N21705 N17551 N17039                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             N13089-0 N18101-0 N1248-0 N26273-0 N12770-1 N1132-0 N13649-0\n",
            "1  3  U11552   11/11/2019 1:03:52 PM                                                                                                                                                                                                                                                                                                                                                                                                                                 N2139                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               N18390-0 N10537-0 N23967-1\n",
            "2  4  U68381   11/11/2019 6:44:05 AM  N27420 N11621 N25416 N25457 N5124 N11751 N11751 N16918 N5124 N27030 N20782 N13820 N24638 N4589 N12871 N1038 N20703 N23344 N28493 N6122 N12242 N11657 N28154 N10989 N3994 N14776 N28493 N26260 N8067 N10289 N21020 N10031 N10636 N5638 N26159 N2158 N3842 N23750 N2659 N13700 N27591 N27304 N28322 N1423 N9841 N425 N4384 N23628 N2908 N952 N21545 N828 N18620 N25310 N828 N26748 N10698 N16242 N5740 N10231 N1156 N9807 N6596 N14201  N15660-0 N18609-0 N2831-0 N5677-0 N19010-0 N1502-0 N19215-0 N6753-0 N21602-0 N6078-0 N15525-0 N16169-0 N4022-0 N16856-0 N16309-0 N28261-0 N425-0 N7910-0 N7114-0 N18390-1 N25787-0 N27982-0 N8420-0 N8256-0 N13359-0 N843-0 N4894-0 N26156-0 N24345-0 N18780-0 N15947-0 N23392-0 N5675-0 N26467-0 N9781-0 N3860-0 N1796-0 N23511-0 N14030-0 N10264-0 N27379-0 N23603-0 N10821-0 N21335-0 N26308-0 N18786-0 N3403-0 N16833-0 N5780-0 N6823-0 N10151-0 N19679-0 N15858-0 N16296-0 N7090-0 N21194-0 N23860-0 N6342-0 N1313-0 N2342-0 N16945-0 N6677-0 N8260-0 N21101-0 N18162-0 N19030-0 N19577-0 N8427-0 N10552-0 N1127-0\n",
            "3  5  U52303   11/12/2019 3:11:52 AM                                                                                                                                                                                                                                                                                                                                                                                                                          N1332 N12667                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         N15645-0 N7911-1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Training"
      ],
      "metadata": {
        "id": "XVv1Ku-UmAK7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hypter parameters\n",
        "hparams = prepare_hparams(yaml_file, \n",
        "                          wordEmb_file=wordEmb_file,\n",
        "                          wordDict_file=wordDict_file, \n",
        "                          userDict_file=userDict_file,\n",
        "                          batch_size=batch_size,\n",
        "                          epochs=epochs)\n",
        "print(hparams)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c7E-yjVTjDZ7",
        "outputId": "b361dc5c-1dff-4ea4-e96c-e8807b2c3950"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HParams object with values {'support_quick_scoring': True, 'dropout': 0.2, 'attention_hidden_dim': 200, 'head_num': 4, 'head_dim': 100, 'filter_num': 400, 'window_size': 3, 'vert_emb_dim': 100, 'subvert_emb_dim': 100, 'gru_unit': 400, 'type': 'ini', 'user_emb_dim': 50, 'learning_rate': 0.0001, 'optimizer': 'adam', 'epochs': 5, 'batch_size': 32, 'show_step': 100000, 'title_size': 30, 'his_size': 50, 'data_format': 'news', 'npratio': 4, 'metrics': ['group_auc', 'mean_mrr', 'ndcg@5;10'], 'word_emb_dim': 300, 'cnn_activation': 'relu', 'model_type': 'lstur', 'loss': 'cross_entropy_loss', 'wordEmb_file': '/tmp/tmpp60vqkoq/utils/embedding.npy', 'wordDict_file': '/tmp/tmpp60vqkoq/utils/word_dict.pkl', 'userDict_file': '/tmp/tmpp60vqkoq/utils/uid2index.pkl'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# model initializaing using the hyperparameters\n",
        "model = LSTURModel(hparams, MINDIterator, seed=seed)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-aZujII9mUD7",
        "outputId": "18af23eb-9672-4d60-d890-e715e44977c9"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tensor(\"conv1d/Relu:0\", shape=(None, 30, 400), dtype=float32)\n",
            "Tensor(\"att_layer2/Sum_1:0\", shape=(None, 400), dtype=float32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/optimizers/optimizer_v2/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.run_eval(valid_news_file, valid_behaviors_file))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JT4SkhZImZYB",
        "outputId": "c89de212-6dc4-4e68-a9a7-7773f4ed036b"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r0it [00:00, ?it/s]/usr/local/lib/python3.9/dist-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n",
            "586it [00:10, 54.94it/s] \n",
            "236it [00:08, 27.87it/s]\n",
            "7538it [00:00, 8271.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'group_auc': 0.5201, 'mean_mrr': 0.2214, 'ndcg@5': 0.2292, 'ndcg@10': 0.2912}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "model.fit(train_news_file, train_behaviors_file, valid_news_file, valid_behaviors_file)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eZ5sq52NmeVP",
        "outputId": "017dd6ba-69bf-4a7b-8750-46890f02e50e"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1086it [03:19,  5.44it/s]\n",
            "586it [00:00, 707.51it/s]\n",
            "236it [00:06, 34.21it/s]\n",
            "7538it [00:01, 4366.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "at epoch 1\n",
            "train info: logloss loss:1.490429998322306\n",
            "eval info: group_auc:0.5931, mean_mrr:0.257, ndcg@10:0.3458, ndcg@5:0.2824\n",
            "at epoch 1 , train time: 199.5 eval time: 15.9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1086it [03:07,  5.79it/s]\n",
            "586it [00:00, 721.44it/s]\n",
            "236it [00:06, 34.16it/s]\n",
            "7538it [00:00, 8904.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "at epoch 2\n",
            "train info: logloss loss:1.4036107010604268\n",
            "eval info: group_auc:0.62, mean_mrr:0.2787, ndcg@10:0.371, ndcg@5:0.3063\n",
            "at epoch 2 , train time: 187.5 eval time: 15.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1086it [03:07,  5.80it/s]\n",
            "586it [00:00, 687.19it/s]\n",
            "236it [00:06, 33.90it/s]\n",
            "7538it [00:00, 8924.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "at epoch 3\n",
            "train info: logloss loss:1.3566746225453652\n",
            "eval info: group_auc:0.6313, mean_mrr:0.2919, ndcg@10:0.3834, ndcg@5:0.3227\n",
            "at epoch 3 , train time: 187.1 eval time: 15.1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1086it [03:06,  5.82it/s]\n",
            "586it [00:00, 724.82it/s]\n",
            "236it [00:06, 33.73it/s]\n",
            "7538it [00:00, 8486.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "at epoch 4\n",
            "train info: logloss loss:1.324074923992157\n",
            "eval info: group_auc:0.6343, mean_mrr:0.2898, ndcg@10:0.3829, ndcg@5:0.3201\n",
            "at epoch 4 , train time: 186.7 eval time: 15.3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1086it [03:07,  5.80it/s]\n",
            "586it [00:01, 492.45it/s]\n",
            "236it [00:06, 35.22it/s]\n",
            "7538it [00:00, 8976.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "at epoch 5\n",
            "train info: logloss loss:1.2885026645814077\n",
            "eval info: group_auc:0.6409, mean_mrr:0.2971, ndcg@10:0.3916, ndcg@5:0.3265\n",
            "at epoch 5 , train time: 187.4 eval time: 16.1\n",
            "CPU times: user 18min 49s, sys: 52.2 s, total: 19min 42s\n",
            "Wall time: 17min 5s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<recommenders.models.newsrec.models.lstur.LSTURModel at 0x7f8aa8f5b490>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "res_syn = model.run_eval(valid_news_file, valid_behaviors_file)\n",
        "print(res_syn)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T5yNZrahm476",
        "outputId": "d2b373ea-ee0c-4b6d-b236-b96d09b74ca2"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "586it [00:00, 625.97it/s]\n",
            "236it [00:06, 35.43it/s]\n",
            "7538it [00:01, 4977.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'group_auc': 0.6409, 'mean_mrr': 0.2971, 'ndcg@5': 0.3265, 'ndcg@10': 0.3916}\n",
            "CPU times: user 14.8 s, sys: 1.81 s, total: 16.6 s\n",
            "Wall time: 15.8 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sb.glue(\"res_syn\", res_syn)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "Ct0zApe9mz3G",
        "outputId": "a30a6e82-fd41-4f2c-d5c3-c6850c7d35df"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/scrapbook.scrap.json+json": {
              "name": "res_syn",
              "data": {
                "group_auc": 0.6409,
                "mean_mrr": 0.2971,
                "ndcg@5": 0.3265,
                "ndcg@10": 0.3916
              },
              "encoder": "json",
              "version": 1
            }
          },
          "metadata": {
            "scrapbook": {
              "name": "res_syn",
              "data": true,
              "display": false
            }
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Saving the model"
      ],
      "metadata": {
        "id": "D2fixQVQm9i7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Setting the path for the model\n",
        "model_path = os.path.join(data_path, \"model\")\n",
        "\n",
        "#Creating the directory if it does not exist\n",
        "os.makedirs(model_path, exist_ok=True)\n",
        "\n",
        "#Saving the weights of the model at the specified path\n",
        "model.model.save_weights(os.path.join(model_path, \"lstur_ckpt\"))"
      ],
      "metadata": {
        "id": "ZIlsSLSpm-oI"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Predictions"
      ],
      "metadata": {
        "id": "KXfxJLlLqe7K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the pre-trained model and evaluate on the validation data\n",
        "group_impr_indexes, group_labels, group_preds = model.run_fast_eval(valid_news_file, valid_behaviors_file)\n",
        "\n",
        "# Open a file to write the prediction results\n",
        "with open(os.path.join(data_path, 'prediction.txt'), 'w') as f:\n",
        "    # Iterate through each impression index and the corresponding predictions\n",
        "    for impr_index, preds in tqdm(zip(group_impr_indexes, group_preds)):\n",
        "        # Increment the impression index by 1 (since impression indexes start from 0)\n",
        "        impr_index += 1\n",
        "        # Compute the predicted ranking for the current set of predictions\n",
        "        pred_rank = (np.argsort(np.argsort(preds)[::-1]) + 1).tolist()\n",
        "        # Format the predicted ranking as a string in the required format\n",
        "        pred_rank = '[' + ','.join([str(i) for i in pred_rank]) + ']'\n",
        "        # Write the impression index and predicted ranking to the output file\n",
        "        f.write(' '.join([str(impr_index), pred_rank])+ '\\n')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o9RMpQt9m_Tv",
        "outputId": "f6c729f8-5eb4-4ef5-fd86-db1a4d4e3c31"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "586it [00:00, 601.25it/s]\n",
            "236it [00:06, 36.56it/s]\n",
            "7538it [00:02, 3624.13it/s]\n",
            "7538it [00:00, 27741.56it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a new zip file to store the prediction file\n",
        "f = zipfile.ZipFile(os.path.join(data_path, 'prediction.zip'), 'w', zipfile.ZIP_DEFLATED)\n",
        "\n",
        "# Add the prediction file to the zip archive with a specified arcname\n",
        "f.write(os.path.join(data_path, 'prediction.txt'), arcname='prediction.txt')\n",
        "\n",
        "# Close the zip file\n",
        "f.close()\n"
      ],
      "metadata": {
        "id": "VOI_grAHqnMA"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summary and Conclusion\n",
        "In this notebook, we have explored the use of NLP and recommendation systems to build a news recommendation engine. Using the Microsoft MIND dataset and the Microsoft Recommenders library, we implemented the LSTUR algorithm to learn user representations and predict personalized news recommendations for each user.\n",
        "\n",
        "After training the model, we obtained a group_auc of 0.6409, mean_mrr of 0.2971, ndcg@5 of 0.3265, and ndcg@10 of 0.3916. These evaluation metrics indicate that our model is performing reasonably well in terms of accuracy and personalization of recommendations.\n",
        "\n",
        "In conclusion, the use of NLP and recommendation systems is a powerful tool for building news recommendation engines that can provide personalized recommendations to users. Our implementation of the LSTUR algorithm has shown promising results, and there is room for further improvement and exploration in this field. With the growing importance of news consumption and the abundance of news articles available online, news recommendation engines have become increasingly important in helping users find the content that is most relevant to their interests."
      ],
      "metadata": {
        "id": "90f1blTDglnu"
      }
    }
  ]
}