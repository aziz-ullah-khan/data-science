{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk import sent_tokenize, word_tokenize, bigrams, trigrams\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet as wn\n",
    "from collections import Counter, defaultdict\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, ConfusionMatrixDisplay,classification_report\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "import os, random, math\n",
    "from string import punctuation\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import itertools\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading files & Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trining files directory\n",
    "TRAINING_DIR=os.getcwd()+\"\\\\Holmes_Training_Data\" #Parent directory for the training corpus\n",
    "\n",
    "# Get Training files\n",
    "def get_training(training_dir=TRAINING_DIR):\n",
    "    filenames=os.listdir(training_dir)\n",
    "    print(f\"There are {len(filenames)} files in the training directory: {training_dir}\")\n",
    "    return(filenames)\n",
    "\n",
    "training_files=get_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get Test Data\n",
    "test_questions = pd.read_csv('testing_data.csv')\n",
    "test_answers = pd.read_csv('test_answer.csv')\n",
    "\n",
    "# Display Top 5 test questions\n",
    "test_questions.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization\n",
    "# Removing punctuation (!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~)\n",
    "# Getting the base words through lematization\n",
    "def pre_process(sentence):\n",
    "    return [WordNetLemmatizer().lemmatize(token).lower() for token in word_tokenize(sentence) if token not in punctuation]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process text\n",
    "def text_process(text):\n",
    "    # Pre-process text data with sentence tokenization and collect words  \n",
    "    for sentence in sent_tokenize(text):\n",
    "        #pre-process sentence\n",
    "        tokens_processed = pre_process(sentence)\n",
    "        #collect tokens for word2vec\n",
    "        tokens_wv.append(tokens_processed)\n",
    "        #collect all tokens\n",
    "        tokens_list.extend(tokens_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read all the files from the Training directory, extract the text and process\n",
    "def processfiles(training_dir=TRAINING_DIR, files = []):\n",
    "        for afile in files:\n",
    "            print(f\"Processing {afile}\")\n",
    "            print(\"Path\" +os.path.join(training_dir, afile))\n",
    "            try:\n",
    "                with open(os.path.join(training_dir, afile)) as instream:\n",
    "                    text_process(instream.read())\n",
    "            except UnicodeDecodeError:\n",
    "                print(\"UnicodeDecodeError processing {}: ignoring file\".format(afile))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#pre-process question columns to remove punctuation, tokenize and get base word (lemmatization)\n",
    "test_questions.question = test_questions.question.apply(lambda question: pre_process(question))\n",
    "\n",
    "#pre-process columns a,b,c,d,e\n",
    "test_questions['a)']=test_questions['a)'].apply(lambda option: pre_process(option))\n",
    "test_questions['b)']=test_questions['b)'].apply(lambda option: pre_process(option))\n",
    "test_questions['c)']=test_questions['c)'].apply(lambda option: pre_process(option))\n",
    "test_questions['d)']=test_questions['d)'].apply(lambda option: pre_process(option))\n",
    "test_questions['e)']=test_questions['e)'].apply(lambda option: pre_process(option))\n",
    "\n",
    "#Display\n",
    "test_questions.head(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uni-gram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uni-gram model \n",
    "def uni_gram(tokens_list):\n",
    "    \n",
    "    global unigram_model\n",
    "    for token in tokens_list:\n",
    "        unigram_model[token]=unigram_model.get(token,0)+1\n",
    "    #with probilities\n",
    "    unigram_model={k:v/sum(unigram_model.values()) for (k,v) in unigram_model.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bi-gram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bi-gram model \n",
    "def bi_gram(tokens_list):\n",
    "    for w1, w2 in bigrams(tokens_list, pad_right=True, pad_left=True):\n",
    "        bigram_model[(w1)][w2] += 1\n",
    "\n",
    "    # Let's transform the counts to probabilities\n",
    "    for w1 in bigram_model:\n",
    "        total_count = float(sum(bigram_model[w1].values()))\n",
    "        for w2 in bigram_model[w1]:\n",
    "            bigram_model[w1][w2] /= total_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tri-gram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tri_gram(tokens_list):\n",
    "    for w1, w2, w3 in trigrams(tokens_list, pad_right=True, pad_left=True):\n",
    "        trigram_model[(w1, w2)][w3] += 1\n",
    "\n",
    "    # Let's transform the counts to probabilities\n",
    "    for w1_w2 in trigram_model:\n",
    "        total_count = float(sum(trigram_model[w1_w2].values()))\n",
    "        for w3 in trigram_model[w1_w2]:\n",
    "            trigram_model[w1_w2][w3] /= total_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create a placeholder for models\n",
    "unigram_model = {}\n",
    "bigram_model = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "trigram_model = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "\n",
    "#list for word2vec\n",
    "tokens_wv = []\n",
    "#all tokens list\n",
    "tokens_list = []\n",
    "\n",
    "# process all files\n",
    "processfiles(TRAINING_DIR, training_files)\n",
    "\n",
    "#unigram model training\n",
    "uni_gram(tokens_list)\n",
    "\n",
    "#bigram model training\n",
    "bi_gram(tokens_list)\n",
    "\n",
    "#trigram model training\n",
    "tri_gram(tokens_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#word2vector model training\n",
    "word2vec_model = gensim.models.Word2Vec(tokens_wv, min_count = 1, vector_size=100, window=5, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save variables for future processing\n",
    "pickle.dump(tokens_wv, open('variables/tokens_wv', 'wb'), True)\n",
    "pickle.dump(tokens_list, open('variables/tokens_list', 'wb'), True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pickle.load(open('variables/tokens_list', 'rb'))\n",
    "#pickle.load(open('variables/tokens_wv', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the prevoius two words from the blank \n",
    "def get_previous_words(question):\n",
    "    first_word = ''\n",
    "    second_word = ''\n",
    "    indx_blank = question.index('_____')\n",
    "    if question[indx_blank-2]:\n",
    "        first_word = question[indx_blank-2]\n",
    "        second_word = question[indx_blank-1]\n",
    "    elif question[indx_blank-1]:\n",
    "        second_word = question[indx_blank-1]\n",
    "    return first_word, second_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding the cosien similrity between two words w1 & w2\n",
    "def cosine_similarity(word2vec_model, w1, w2):\n",
    "    if word2vec_model.wv.has_index_for(w1) and word2vec_model.wv.has_index_for(w2):\n",
    "        cosine_similarity = word2vec_model.wv.similarity(w1, w2)\n",
    "    else:\n",
    "        cosine_similarity = 0\n",
    "    return cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get synonyms with wordnet\n",
    "def get_synonyms(word):\n",
    "    syn = set()\n",
    "    for synset in wn.synsets(word):\n",
    "        for lemma in synset.lemmas():\n",
    "            syn.add(lemma.name())\n",
    "            \n",
    "    syn.add(word)\n",
    "    return list(syn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get probability of the option\n",
    "def get_probability(first_word, second_word, option_word, model):\n",
    "    prob = 0\n",
    "    try:\n",
    "        if model == 'bigram':\n",
    "            prob = dict(bigram_model[second_word])[option_word]\n",
    "        elif model == 'trigram':\n",
    "            prob = dict(trigram_model[first_word, second_word])[option_word]\n",
    "    except:\n",
    "        prob = 0\n",
    "    return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# answer prediction with trigram, word2vec & wordne\n",
    "def predict_answer(question, word2vec_model, options, model, question_no, total_questions, is_w2v_wn):\n",
    "    print(f\"Processing question No. {question_no+1} out of {total_questions}\")\n",
    "    answer_list = ['a', 'b', 'c', 'd', 'e']\n",
    "    max_index = -1\n",
    "    first_word, second_word = get_previous_words(question)\n",
    "    # Join all the lists in one list\n",
    "    options = list(itertools.chain.from_iterable(options))\n",
    "    if not is_w2v_wn:\n",
    "        max_prob = 0\n",
    "        if model=='unigram':\n",
    "            # get random values from unigram \n",
    "            unigram_list = list(np.random.choice(list(unigram_model), size=5))\n",
    "        for indx in range(5):\n",
    "            if model =='unigram':\n",
    "                if options[indx] in unigram_list:\n",
    "                    max_index = indx\n",
    "            elif model == 'bigram':\n",
    "                # here second word is basically the first word for bigram\n",
    "                prob = get_probability(first_word, second_word, options[indx], 'bigram')\n",
    "                if prob > max_prob:\n",
    "                    max_prob = prob\n",
    "                    max_index = indx\n",
    "            elif model == 'trigram':\n",
    "                prob = prob = get_probability(first_word, second_word, options[indx], 'trigram')\n",
    "                if prob > max_prob:\n",
    "                    max_prob = prob\n",
    "                    max_index = indx\n",
    "        return answer_list[max_index]\n",
    "    else:\n",
    "        max_cosine_similarity = -float(\"inf\")\n",
    "        for indx in range(5):\n",
    "            if model =='unigram':\n",
    "                predict_possible_answers= list(np.random.choice(list(unigram_model), size=10))\n",
    "            elif model == 'bigram':\n",
    "                # handle scenario where we don't have match of words in bigram \n",
    "                if len(list(bigram_model[second_word])) > 0:\n",
    "                    # here second word is basically the first word for bigram\n",
    "                    predict_possible_answers= list(np.random.choice(list(bigram_model[second_word]), size=10))\n",
    "                else:\n",
    "                    predict_possible_answers = []\n",
    "            elif model == 'trigram':\n",
    "                if len(list(trigram_model[first_word, second_word])) > 0:\n",
    "                    # here second word is basically the first word for bigram\n",
    "                    predict_possible_answers= list(np.random.choice(list(trigram_model[first_word, second_word]), size=10))\n",
    "                else:\n",
    "                    predict_possible_answers = [] \n",
    "            synonyms_list = get_synonyms(options[indx])[:5]\n",
    "            for i in range(len(predict_possible_answers)):\n",
    "                for j in range(len(synonyms_list)):\n",
    "                    cos_similarity = cosine_similarity(word2vec_model, predict_possible_answers[i], synonyms_list[j])\n",
    "                    if cos_similarity > max_cosine_similarity:\n",
    "                        max_cosine_similarity = cos_similarity\n",
    "                        max_index = indx\n",
    "        return answer_list[max_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model prediction -- uni-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#uni-gram predictions\n",
    "predicted_answers = [predict_answer(test_questions['question'][i], word2vec_model, test_questions.iloc[i,-5:], 'unigram', i, test_questions.shape[0], False) for i in range(test_questions.shape[0])]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation -- uni-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display confusion Matrix\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix(test_answers['answer'], predicted_answers),\n",
    "                              display_labels=['a', 'b', 'c', 'd', 'e'])\n",
    "\n",
    "disp = disp.plot(cmap='YlOrRd')\n",
    "plt.title(\"Confusion matrix - uni-gram\")\n",
    "plt.show()\n",
    "\n",
    "# Classification report\n",
    "clf_report = classification_report(test_answers['answer'], predicted_answers, target_names=['a', 'b', 'c', 'd', 'e'], output_dict=True)\n",
    "\n",
    "# .iloc[:-1, :] to exclude support\n",
    "sns.heatmap(pd.DataFrame(clf_report).iloc[:-1, :].T, annot=True, cmap=\"YlGnBu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model prediction - bi-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bi-gram predictions\n",
    "predicted_answers = [predict_answer(test_questions['question'][i], word2vec_model, test_questions.iloc[i,-5:], 'bigram', i, test_questions.shape[0], False) for i in range(test_questions.shape[0])]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation -- bi-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display confusion Matrix\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix(test_answers['answer'], predicted_answers),\n",
    "                              display_labels=['a', 'b', 'c', 'd', 'e'])\n",
    "\n",
    "disp = disp.plot(cmap='YlOrRd')\n",
    "plt.title(\"Confusion matrix - bi-gram\")\n",
    "plt.show()\n",
    "\n",
    "# Classification report\n",
    "clf_report = classification_report(test_answers['answer'], predicted_answers, target_names=['a', 'b', 'c', 'd', 'e'], output_dict=True)\n",
    "\n",
    "# .iloc[:-1, :] to exclude support\n",
    "sns.heatmap(pd.DataFrame(clf_report).iloc[:-1, :].T, annot=True, cmap=\"YlGnBu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model prediction -- tri-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tri-gram predictions\n",
    "predicted_answers = [predict_answer(test_questions['question'][i], word2vec_model, test_questions.iloc[i,-5:], 'trigram', i, test_questions.shape[0], False) for i in range(test_questions.shape[0])]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation -- tri-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display confusion Matrix\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix(test_answers['answer'], predicted_answers),\n",
    "                              display_labels=['a', 'b', 'c', 'd', 'e'])\n",
    "\n",
    "disp = disp.plot(cmap='YlOrRd')\n",
    "plt.title(\"Confusion matrix - tri-gram\")\n",
    "plt.show()\n",
    "\n",
    "# Classification report\n",
    "clf_report = classification_report(test_answers['answer'], predicted_answers, target_names=['a', 'b', 'c', 'd', 'e'], output_dict=True)\n",
    "\n",
    "# .iloc[:-1, :] to exclude support\n",
    "sns.heatmap(pd.DataFrame(clf_report).iloc[:-1, :].T, annot=True, cmap=\"YlGnBu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model prediction with uni-gram with wordnet & word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#uni-gram predictions\n",
    "predicted_answers = [predict_answer(test_questions['question'][i], word2vec_model, test_questions.iloc[i,-5:], 'unigram', i, test_questions.shape[0], True) for i in range(test_questions.shape[0])]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation -- uni-gram with wordnet & word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display confusion Matrix\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix(test_answers['answer'], predicted_answers),\n",
    "                              display_labels=['a', 'b', 'c', 'd', 'e'])\n",
    "\n",
    "disp = disp.plot(cmap='YlOrRd')\n",
    "plt.title(\"Confusion matrix - uni-gram\")\n",
    "plt.show()\n",
    "\n",
    "# Classification report\n",
    "clf_report = classification_report(test_answers['answer'], predicted_answers, target_names=['a', 'b', 'c', 'd', 'e'], output_dict=True)\n",
    "\n",
    "# .iloc[:-1, :] to exclude support\n",
    "sns.heatmap(pd.DataFrame(clf_report).iloc[:-1, :].T, annot=True, cmap=\"YlGnBu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model prediction -- bi-gram with wordnet & word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bi-gram predictions\n",
    "predicted_answers = [predict_answer(test_questions['question'][i], word2vec_model, test_questions.iloc[i,-5:], 'bigram', i, test_questions.shape[0], True) for i in range(test_questions.shape[0])]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation -- bi-gram with wordnet & word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display confusion Matrix\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix(test_answers['answer'], predicted_answers),\n",
    "                              display_labels=['a', 'b', 'c', 'd', 'e'])\n",
    "\n",
    "disp = disp.plot(cmap='YlOrRd')\n",
    "plt.title(\"Confusion matrix - bi-gram\")\n",
    "plt.show()\n",
    "\n",
    "# Classification report\n",
    "clf_report = classification_report(test_answers['answer'], predicted_answers, target_names=['a', 'b', 'c', 'd', 'e'], output_dict=True)\n",
    "\n",
    "# .iloc[:-1, :] to exclude support\n",
    "sns.heatmap(pd.DataFrame(clf_report).iloc[:-1, :].T, annot=True, cmap=\"YlGnBu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model prediction -- tri-gram with wordnet & word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tri-gram predictions\n",
    "predicted_answers = [predict_answer(test_questions['question'][i], word2vec_model, test_questions.iloc[i,-5:], 'trigram', i, test_questions.shape[0], True) for i in range(test_questions.shape[0])]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation -- tri-gram with wordnet & word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display confusion Matrix\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix(test_answers['answer'], predicted_answers),\n",
    "                              display_labels=['a', 'b', 'c', 'd', 'e'])\n",
    "\n",
    "disp = disp.plot(cmap='YlOrRd')\n",
    "plt.title(\"Confusion matrix - tri-gram\")\n",
    "plt.show()\n",
    "\n",
    "# Classification report\n",
    "clf_report = classification_report(test_answers['answer'], predicted_answers, target_names=['a', 'b', 'c', 'd', 'e'], output_dict=True)\n",
    "\n",
    "# .iloc[:-1, :] to exclude support\n",
    "sns.heatmap(pd.DataFrame(clf_report).iloc[:-1, :].T, annot=True, cmap=\"YlGnBu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
